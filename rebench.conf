# Config file for ReBench
# Config format is YAML (see http://yaml.org/ for detailed spec)

# this run definition will be choosen if no parameters are given to rebench.py
standard_run: CSOM-test

# settings and requirements for statistic evaluation
statistics:
    min_runs: 30
    max_runs: 50
    confidence_level: 0.95
    error_margin: 0.005
    stop_criterium: percentage
    stop_threshold: 5

# definition of benchmark suites
benchmarks:
    CSOM:
        performance_reader: LogPerformance
        # location: /Users/smarr/Projects/PhD/VMs/CSOM
        command: -cp Smalltalk Examples/Benchmarks/%(benchmark)s %(input)s
        input_sizes: [1, 2, 10, 100, 1000]
        benchmarks:
            - All.som
            - Fibonacci.som
            - QuickSort.som
            - Sum.som
            - Ball.som
            - List.som
            - Random.som
            - Towers.som
            - Benchmark.som
            - ListElement.som
            - Recurse.som
            - TowersDisk.som
            - Bounce.som
            - Loop.som
            - Sieve.som
            - TreeNode.som
            - BubbleSort.som
            - Permute.som
            - Sort.som
            - TreeSort.som
            - Dispatch.som
            - Queens.som
            - Storage.som
            - Fannkuch.som:
                extra-args: 6
        ulimit: 300
    CSOM-all:
        performance_reader: LogPerformance
        # location: /Users/smarr/Projects/PhD/VMs/CSOM
        command: -cp Smalltalk Examples/Benchmarks/%(benchmark)s %(input)s
        input_sizes: [1, 2, 10, 100, 1000]
        benchmarks:
            - All.som
        ulimit: 300
    CSOM-loop:
        performance_reader: LogPerformance
        command: -cp Smalltalk Examples/Benchmarks/%(benchmark)s %(input)s
        input_sizes: [1, 2, 10, 100, 1000]
        benchmarks:
            - Loop.som
        ulimit: 300
    Lua:
        performance_reader: LogPerformance
        command: "%(benchmark)s %(input)s "
        input_sizes: [1, 2, 10, 100]
        benchmarks:
            - ackermann.lua:
                extra-args: 6
            - ary3.lua:
                extra-args: 80
            - binarytrees.lua:
                extra-args: 5
            - except.lua:
                extra-args: 600
            - fannkuch-opt.lua:
                extra-args: 7
            - fannkuch.lua:
                extra-args: 6
            - fasta.lua:
                extra-args: 2500
        ulimit: 300

# VMs have a name and are specified by a path and the binary to be executed
virtual_machines:
    CSOM:
        path: /home/smarr/CSOM/smarr/benchmark
        binary: CSOM
    CSOM-combined:
        path: /Users/smarr/Projects/PhD/VMs/CSOM
        binary: CSOM-O3-classic
    Lua-combined:
        path: /home/smarr/Lua/gcc-intermodule-test/src
        binary: lua
    Lua:
        path: /home/smarr/Lua/my-5.1.4/src
        binary: lua

# define the benchmarks to be executed for a re-executable benchmark run
run_definitions:
    CSOMvsLuaProfiling:
        description: >
            This run definition is used to compare some basic properties of CSOM
            and Lua. It is only used to profile, not execution time evaluation.
        actions: profile 
        executions:
            # List of VMs and Benchmarks/Benchmark Suites to be run on them
            #- CSOM:
            #    benchmark: CSOM
            - Lua:
                benchmark: Lua
    CSOM-test:
        description: >
            This run is used for simple testing purpose
        actions: benchmark
        benchmark: CSOM-loop
        input_size: 1
        executions:
            - CSOM
    Combined-Compilation:
        description: >
            This run tries to measure the impact of combined compilation.
        actions: benchmark
        benchmark: Lua
        input_size: 1
        executions:
            - Lua
            - Lua-combined
